---
title: 'ISE537 Homework #2'
author: "Keenan Smith"
date: "3/6/2022"
output: 
  html_document:
    theme:
      bootswatch: pulse
---

"Built with R Version `r getRversion()`"
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem #1
1) Prediction is the only objective of multiple linear regression. 
- False
2) In multiple linear regression, we study the relationship between a single response variable and several predicting quantitative and/or qualitative variables.
- True
3) A multiple linear regression model contains 5 quantitative predicting variables and an intercept. The number of parameters to estimate is 6. 
- True
4) Given a qualitative predicting variable with 7 categories in a linear regression model with intercept, 7 dummy variables need to be included in the model. 
- False
5) In multiple linear regression, quantitative variables can be transformed into qualitative or categorical variables. 
- False
6) The estimated variance of the error terms of a multiple linear regression model with intercept can be obtained by summing up the squared residuals and dividing that by n ‚Äì p, where n is the sample size and p is the number of predictors. 
- False
7) In a first-order multiple linear regression model, the estimated regression coefficient corresponding to a quantitative predicting variable is interpreted as the estimated expected change in the response variable when there is a change of one unit in the corresponding predicting variable holding all other predictors fixed. 
- True 
8) In multiple linear regression, the estimation of the variance of the error terms is unnecessary for making statistical inference on the regression coefficients. 
- False
9) In multiple linear regression, the sampling distribution used for estimating confidence intervals for the regression coefficients is the normal distribution. 
- True 
10) The estimated regression coefficients obtained by using the method of least squares are biased estimators of the true regression coefficients.
- False
11) Conducting t-tests on each parameter ùõΩ is preferable to an F-test when testing the overall significance of a multiple linear regression model. 
- False
12) A partial F-test can be used to test the null hypothesis that the regression coefficients associated with a subset of the predicting variables in a multiple linear regression model are all equal to zero. 
- True
13) In multiple linear regression, if the F-test statistic is greater than the appropriate F-critical value, then at least one of the slope coefficients is significantly different from zero at the given significance level. 
- True
14) In multiple linear regression, the uncertainty of the prediction of a new response comes only from the newness of the observation.
- False

## Library Import
```{r include=FALSE}
library(tidyverse)
library(visreg)
library(parameters)
```

## Import Dataset and Tidy
```{r}
fram <- read_csv("data/fram.csv")
fram <-
  fram %>%
  rename(
    sex = SEX,
    age = AGE,
    sysbp = SYSBP,
    cursmoke = CURSMOKE,
    bmi = BMI
  ) %>%
  mutate(
    sex_fact = as.factor(sex),
    cursmoke_fact = as.factor(cursmoke)
  )

fram_mod <-
  fram %>%
  filter(bmi >= 30)
```

## Model #1
```{r}
# Fitting Linear Regression to the Model
lm_fit_fram <- lm(sysbp ~ age + sex_fact + cursmoke_fact + bmi, data = fram)
# Summary Data
summary(lm_fit_fram)
model_parameters(lm_fit_fram, summary = TRUE)

# Summary Plots
plot(lm_fit_fram)
visreg(lm_fit_fram)
performance::check_model(lm_fit_fram)

# Residual Tibble
fram_residuals <- tibble(residuals = lm_fit_fram$residuals)
```

15) How many regression coefficients including the intercept are statistically significant at the significance level 0.01? 
- Three
16) What is the residual associated with the last observation in the data set? 
- -30.576
17) What is the interpretation of the estimated regression coefficient corresponding to the BMI predicting variable?
- Systolic blood pressure increases by approximately 1.5634 mmHg with a one unit increase in BMI controlling for sex, age, and smoking habit.
18) What is the approximate estimated variance of the error terms? 
- 19.75

## Model BMI > 30
```{r}
# Fitting the Linear Regression to the Data
lm_fit_fram_mod <- lm(sysbp ~ age + sex_fact + cursmoke_fact + bmi, data = fram_mod)

# Summary Data
summary(lm_fit_fram_mod)
model_parameters(lm_fit_fram_mod, summary = TRUE)

# Summary Plots
plot(lm_fit_fram_mod)
visreg(lm_fit_fram_mod)
```

19) If you only include the obese population (BMI>=30) in the sample of data and perform the regression analysis again, which of the regression coefficients (including intercept) are statistically significant at the significance level 0.01?
- None
20) How are the fitted models between overall population and obese population different?
- All of the above
21) For a multiple linear regression model to be a good fit, we need the linearity assumption to hold for at least one of the predicting variables. 
- False
22) In multiple linear regression, we can assess the assumption of constant-variance by plotting the standardized residuals against fitted values.
- True 
23) In multiple linear regression, we could diagnose the normality assumption by using the normal probability plot (qq plot). 
- True 
24) In multiple linear regression, the proportion of variability in the response variable that is explained by the predicting variables is called adjusted ùëÖ2 
- False
25) In multiple linear regression, the adjusted ùëÖ2 can be used to compare models, and its value will always be greater than or equal to that of ùëÖ2 
- False
26) We cannot estimate the regression coefficients of a multiple linear regression model if the predicting variables are linearly independent.
- False
27) In multiple linear regression, when using very large samples, relying on the p-values associated to the traditional hypothesis test with ùêª1:ùõΩùëó‚â†0 can lead to misleading conclusions on the statistical significance of the regression coefficients. 
- True

# Problem #2
```{r include=FALSE}
library(tidymodels)
library(tidyverse)
tidymodels_prefer()
```

## Import Dataset and Tidy
```{r}
fishfull <- read_csv("data/Fish.csv")

# Initial Data Tidy
fishfull <-
  fishfull %>%
  rename(
    weight = Weight,
    species = Species,
    body_height = `Body Height`,
    total_length = `Total Length`,
    diagonal_length = `Diagonal Length`,
    height = Height,
    width = Width
  )

row_cnt <- nrow(fishfull)

# Testing Dataset
fishtest <-
  fishfull %>%
  slice((row_cnt - 9):row_cnt) %>%
  arrange(species)

# Training Dataset
fish <-
  fishfull %>%
  slice(1:(row_cnt - 10)) %>%
  arrange(species)
```

## Exploratory Data Analysis
```{r}
ggplot(data = fish, aes(x = species, y = weight)) +
  geom_boxplot() +
  theme_dark()
ggplot(data = fish, aes(x = body_height, y = weight)) +
  geom_point() +
  theme_dark()
ggplot(data = fish, aes(x = total_length, y = weight)) +
  geom_point() +
  theme_dark()
ggplot(data = fish, aes(x = diagonal_length, y = weight)) +
  geom_point() +
  theme_dark()
ggplot(data = fish, aes(x = height, y = weight)) +
  geom_point() +
  theme_dark()
ggplot(data = fish, aes(x = width, y = weight)) +
  geom_point() +
  theme_dark()

fish_numeric <-
  fish %>%
  select(-species)

round(cor(fish_numeric), digits = 3)
```

### Question 1: Exploratory Data Analysis
a. Create a box plot comparing the response variable, Weight, across the multiple species. Based on this box plot, does there appear to be a relationship between the predictor and the response?
- There does appear to be a relationship between the predictor and the variable with different species having different weights depending on what species they are. 
b. Create plots of the response, Weight, against each quantitative predictor, namely Body.Height, Total.Length, Diagonal.Length, Height, and Width. Describe the general trend of each plot. Are there any potential outliers?
- There general trend of each plot 
c. Display the correlations between each of the variables. Interpret the correlations in the context of the relationships of the predictors to the response.
- There is a reasonable correlation between each of the factors with the weakest being between Height and Weight and Height and its fellow predictors.
d. Based on this exploratory analysis, is it reasonable to assume a multiple linear regression model for the relationship between Weight and the predictor variables?
- Yes, it is reasonable to assume

## Model 1
```{r}
# Recipe Creation for Tidy Models
model1_rec <-
  recipe(weight ~ ., data = fish) %>%
  step_dummy(all_nominal_predictors())

# Creating Linear Regression Model Engine
lm_model <-
  linear_reg() %>%
  set_engine("lm")

# Creating Workflow for Tidy Models
lm_wflow_m1 <-
  workflow() %>%
  add_recipe(model1_rec) %>%
  add_model(lm_model)

# Fitting the Linear Regression to the Data
model1 <- fit(lm_wflow_m1, fish)

# Tidying the Fitted Data
model1 %>%
  extract_fit_parsnip() %>%
  tidy()

# Bringing the Model back into Base R to use Base R functions on the Model
model1_eng <- model1 %>% extract_fit_engine()

plot(model1_eng)
model_parameters(model1_eng)

fish_residuals <- tibble(residuals = model1_eng$residuals)

fish_residuals <-
  fish %>%
  bind_cols(fish_residuals)

performance::check_model(model1_eng)
```

### Question 2: Fitting the Multiple Linear Regression Model
a. Build a multiple linear regression model, called model1, using the response and all predictors. Display the summary table of the model.
b. Is the overall regression significant at an ùõº level of 0.01?
- The overall f-test p-value is significant at a level of 0.01
c. What is the coefficient estimate for Body.Height? Interpret this coefficient.
- The body height coefficient is -176.87 which all other predictors remaining constant predicts that an increase in body height will result in a decrease of 177 in weight in grams. It is also significant in this current model with a p-value of .005.
d. What is the coefficient estimate for the Species category Parkki? Interpret this coefficient.
- The coefficient for the Parkki species with the Bream being the standard fish is 79.34. This says that all things being equal, comparared to the weight of an equivalent Bream, the Parkki should be 79.34 grams heavier than the Bream. However, the p-value is .55 which states that the regressor may be equal to 0 statistically. The 95% confidence interval for the Parkki does contain 0 which supports the conclusion to not reject the null hypothesis.

### Question 3: Checking for Outliers
a. Create a plot for the Cook‚Äôs Distances. Using a threshold Cook‚Äôs Distance of 1, identify the row numbers of any outliers.
- The single outlier with Cooks distance > 1 is row 119 of the dataset. This is a particularly heavy Roach Fish.
b. Remove the outlier(s) from the data set and create a new model, called model2, using all predictors with Weight as the response. Display the summary of this model.

## Model 2
```{r}
fish2 <- fish[-119, ]

# Recipe Creation for Tidy Models
model2_rec <-
  recipe(weight ~ ., data = fish2) %>%
  step_dummy(all_nominal_predictors())

# Creating Workflow for Tidy Models
lm_wflow_m2 <-
  lm_wflow_m1 %>%
  update_recipe(model2_rec)

# Fitting the Linear Regression to the Data
model2 <- fit(lm_wflow_m2, fish2)

# Tidying the Fitted Data
model2 %>%
  extract_fit_parsnip() %>%
  tidy()

# Bringing the Model back into Base R to use Base R functions on the Model
model2_eng <- model2 %>% extract_fit_engine()

plot(model2_eng)
model_parameters(model2_eng)

fish2_residuals <- tibble(residuals = model2_eng$residuals)
fish2_std_residuals <- tibble(std_residuals = rstandard(model2_eng))
fish2_fitted_values <- tibble(fitted_values = model2_eng$fitted.values)

fish2_residuals <-
  fish2 %>%
  bind_cols(fish2_residuals) %>%
  bind_cols(fish2_std_residuals) %>%
  bind_cols(fish2_fitted_values)

performance::check_model(model2_eng)
```

## Plotting Standardized Residuals
```{r}
# Plotting Std Residuals vs Regressors
ggplot(data = fish2_residuals, aes(x = std_residuals, y = body_height)) +
  geom_point() +
  theme_dark()
ggplot(data = fish2_residuals, aes(x = std_residuals, y = total_length)) +
  geom_point() +
  theme_dark()
ggplot(data = fish2_residuals, aes(x = std_residuals, y = diagonal_length)) +
  geom_point() +
  theme_dark()
ggplot(data = fish2_residuals, aes(x = std_residuals, y = height)) +
  geom_point() +
  theme_dark()
ggplot(data = fish2_residuals, aes(x = std_residuals, y = width)) +
  geom_point() +
  theme_dark()
ggplot(data = fish2_residuals, aes(x = fitted_values, y = std_residuals)) +
  geom_point() +
  theme_dark()

# Histogram and QQ plot for Standardized Residuals
ggplot(data = fish2_residuals, aes(x = std_residuals)) +
  geom_histogram(binwidth = .1)
ggplot(data = fish2_residuals, aes(sample = std_residuals)) +
  geom_qq() +
  geom_abline(lty = 2)
```

### Question 4: Checking Model Assumptions
a. Create scatterplots of the standardized residuals of model2 versus each quantitative predictor. Does the linearity assumption appear to hold for all predictors?
- The scatter plots seem to show a non-linear relationship for the predictors with each having a rough funnel shape.
b. Create a scatter plot of the standardized residuals of model2 versus the fitted values of model2. Does the constant variance assumption appear to hold?
- The residuals vs the fitted values shows a rough u-shape which indicates a non-linear relationship. 
c. Create a histogram and normal QQ plot for the standardized residuals. What conclusions can you draw from these plots?
- The model does not fit the assumptions adequately of a multiple linear regression model. 

## Model 3
```{r}
model3_rec <-
  recipe(weight ~ species + total_length, data = fish2) %>%
  step_dummy(all_nominal_predictors())

# Creating Workflow for Tidy Models
lm_wflow_m3 <-
  lm_wflow_m1 %>%
  update_recipe(model3_rec)

# Fitting the Linear Regression to the Data
model3 <- fit(lm_wflow_m3, fish2)

# Tidying the Fitted Data
model3 %>%
  extract_fit_parsnip() %>%
  tidy()

# Bringing the Model back into Base R to use Base R functions on the Model
model3_eng <- model3 %>% extract_fit_engine()

plot(model3_eng)
model_parameters(model3_eng)

fish3_residuals <- tibble(residuals = model3_eng$residuals)
fish3_std_residuals <- tibble(std_residuals = rstandard(model3_eng))
fish3_fitted_values <- tibble(fitted_values = model3_eng$fitted.values)

fish3_residuals <-
  fish2 %>%
  bind_cols(fish3_residuals) %>%
  bind_cols(fish3_std_residuals) %>%
  bind_cols(fish3_fitted_values)

performance::check_model(model3_eng)
```

### Anova Partial F Test
```{r}
anova(model2_eng, model3_eng)
```

b. Conduct a partial F-test comparing model3 with model2. What can you conclude using an ùõº level of 0.01?
- Partial F-Test comparing Model 2 to Model 3 results in an F statistic of 1.7626 with a P-value of 0.14, this shows that there is not a significant difference between Model 2 and Model 3 and that there is a failure to reject the null hypothesis.

### Residual Analysis Model 3
```{r}
# Plotting Std Residuals vs Regressors

ggplot(data = fish3_residuals, aes(x = std_residuals, y = total_length)) +
  geom_point() +
  see::theme_lucid()

ggplot(data = fish3_residuals, aes(x = fitted_values, y = std_residuals)) +
  geom_point() +
  see::theme_lucid()

# Histogram and QQ plot for Standardized Residuals
ggplot(data = fish3_residuals, aes(x = std_residuals)) +
  geom_histogram(binwidth = .1)
ggplot(data = fish3_residuals, aes(sample = std_residuals)) +
  geom_qq() +
  geom_abline(lty = 2)
```

### Question 6: Reduced Model Residual Analysis
b. Conduct residual analysis for model3 (similar to Q4). Comment on each assumption and whether they hold.
- The residual plots look similar to Model 2 which means that the conclusions remain the same between the two.

```{r}
library(MASS)

bc <- boxcox(weight ~ total_length, data = fish2)
lambda <- bc$x[which.max(bc$y)]
lambda

fish2 <-
  fish2 %>%
  mutate(weight_mod = sqrt(weight))

model4_rec <-
  recipe(weight_mod ~ species + total_length, data = fish2) %>%
  step_dummy(all_nominal_predictors())

# Creating Workflow for Tidy Models
lm_wflow_m4 <-
  lm_wflow_m1 %>%
  update_recipe(model4_rec)

# Fitting the Linear Regression to the Data
model4 <- fit(lm_wflow_m4, fish2)

# Tidying the Fitted Data
model4 %>%
  extract_fit_parsnip() %>%
  tidy()

# Bringing the Model back into Base R to use Base R functions on the Model
model4_eng <- model4 %>% extract_fit_engine()

plot(model4_eng)
model_parameters(model4_eng)

fish4_residuals <- tibble(residuals = model4_eng$residuals)
fish4_std_residuals <- tibble(std_residuals = rstandard(model4_eng))
fish4_fitted_values <- tibble(fitted_values = model4_eng$fitted.values)

fish4_residuals <-
  fish2 %>%
  bind_cols(fish4_residuals) %>%
  bind_cols(fish4_std_residuals) %>%
  bind_cols(fish4_fitted_values)

performance::check_model(model4_eng)
```

### Question 7: Transformation
a. Use model3 to find the optimal lambda, rounded to the nearest 0.5, for a Box-Cox transformation on model3. What transformation, if any, should be applied according to the lambda value? Please ensure you use model3.
- Lambda Value is equal to .4242 which rounds to .5 which means that weight should be transformed by sqrt.
b. Based on the results in (a), create model4 with the appropriate transformation. Display the summary.
- Displayed

```{r}
# Plotting Std Residuals vs Regressors

ggplot(data = fish4_residuals, aes(x = std_residuals, y = total_length)) +
  geom_point() +
  theme_dark()
ggplot(data = fish4_residuals, aes(x = fitted_values, y = std_residuals)) +
  geom_point() +
  theme_dark()

# Histogram and QQ plot for Standardized Residuals
ggplot(data = fish4_residuals, aes(x = std_residuals)) +
  geom_histogram(binwidth = .1)
ggplot(data = fish4_residuals, aes(sample = std_residuals)) +
  geom_qq() +
  geom_abline(lty = 2)
```

c. Perform Residual Analysis on model4. Comment on each assumption. Was the transformation successful/unsuccessful?
- The residuals display linearity, independence, normality and homoscedasticity with no influential outliers. The transformation, with my interpretation would be deemed successful. The model also has a good adjusted R squared. 

## Summary Comparison 
```{r}
summary(model2_eng)
summary(model3_eng)
summary(model4_eng)
```

### Question 8: Model Comparison
a. Using each model summary, compare and discuss the R-squared and Adjusted R-squared of model2, model3, and model4.
- Model 2 has an adjusted R2 of .9335, Model 3 has one of .9321 and Model 4 is .98. This indicates that the transformation implemented in model 4 is an effective model for multiple linear regression in this application. Further study could adjust the model to allow for more predictors in an educated and informed way.

```{r}
test_fish <- predict(model4, new_data = fishtest %>% select(-weight))
test_fish <-
  test_fish %>%
  transmute(predicted_values = .pred^2)

test_fish <- bind_cols(test_fish, fishtest %>% select(weight))
test_fish

# This sets a function for Metric Sets and Assigns it to ames_metrics
# ames_metrics is then called with the df, truth, and estimate to output the tibble.
test_metrics <- metric_set(rmse, rsq, mae)
test_metrics(test_fish, truth = weight, estimate = predicted_values)
```

### Question 9: Estimation and Prediction
a. Estimate Weight for the last 10 rows of data (fishtest) using both model3 and model4. Compare and discuss the mean squared prediction error (MSPE) of both models.
b. Suppose you have found a Perch fish with a Body.Height of 28 cm, and a Total.Length of 32 cm. Using model4, predict the weight on this fish with a 90% prediction interval. Provide an interpretation of the prediction interval
